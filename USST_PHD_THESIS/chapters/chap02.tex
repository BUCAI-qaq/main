

\chapter{理论基础和问题建模}%15页






\section{人形机器人全身控制}
\label{sec:whole_body_control}
人形机器人通常具有高自由度、多关节串并联混合结构，其运动控制问题天然具有强耦合性与高度冗余性。在执行行走、操作、支撑以及复杂交互行为时，机器人需要同时满足动力学一致性、接触稳定性、关节物理约束以及多任务目标之间的协调要求。传统将控制问题分解为若干独立子系统的方法难以在全身层面实现一致、稳定且可扩展的运动生成，因此，全身控制（Whole-Body Control, WBC）作为一种统一协调机器人整体运动与受力行为的控制范式逐渐形成。

全身控制的核心思想在于，将机器人视为一个整体系统，在统一的数学框架下对其所有自由度、任务目标以及物理约束进行联合建模与协调求解。与仅关注局部关节或单一末端执行器的控制方法不同，全身控制显式考虑任务之间的潜在冲突以及系统冗余自由度的分配问题，从而能够在满足关键安全与稳定性约束的前提下，协调完成多目标运动任务。全身控制方法主要分为两类：基于零空间的控制（Null-space WBC）和基于二次规划的控制（QP/SQP WBC）。  


\subsection{基于零空间的多任务优先级控制}

设机器人关节空间自由度为 $q \in \mathbb{R}^n$，任务空间向量为 $x \in \mathbb{R}^m$，满足运动学约束：
\begin{equation}
x = f(q),
\end{equation}
对其求导可得关节速度与任务速度的映射关系：
\begin{equation}
\dot{x} = J(q) \dot{q},
\end{equation}
其中 $J \in \mathbb{R}^{m \times n}$ 为任务雅可比矩阵。当 $n > m$ 时，任务存在冗余自由度，此时雅可比矩阵的右伪逆 $J^\dagger$ 可用于求解最小范数解：
\begin{equation}
\dot{q} = J^\dagger \dot{x}.
\end{equation}

零空间 $\mathcal{N}(J)$ 定义为满足 $\dot{x} = 0$ 的关节速度集合，即：
\begin{equation}
\mathcal{N}(J) = \{\dot{q} \in \mathbb{R}^n \mid J \dot{q} = 0\}.
\end{equation}
对应的零空间投影矩阵为：
\begin{equation}
N = I - J^\dagger J,
\end{equation}
它可以将任意关节速度投影到任务零空间。

对于两个任务 $x_1$ 和 $x_2$，其中 $x_1$ 优先级高于 $x_2$，其关节速度解可迭代表示为：
\begin{align}
\dot{q}_1 &= J_1^\dagger \dot{x}_1 + N_1 \dot{q}_0, \\
\dot{q}_2 &= J_2^\dagger \dot{x}_2 + N_2 \dot{q}_1,
\end{align}
其中 $\dot{q}_0$ 为任意关节速度向量，$N_1$ 为任务1的零空间投影矩阵。通过这种方式，低优先级任务可以在高优先级任务零空间内执行，从而保证高优先级任务不被破坏。对于 $k$ 个任务的情况，可推广为迭代形式：
\begin{equation}
\dot{q} = \dot{q}_k = J_k^\dagger \dot{x}_k + N_k \dot{q}_{k-1}, \quad N_k = I - J_k^\dagger J_k.
\end{equation}

\subsection{基于分层二次规划的多任务控制}

将运动学方程写成最优化形式：
\begin{equation}
\min_{\dot{q}} \| J \dot{q} - \dot{x}_{\text{des}} \|^2,
\end{equation}
其中 $\dot{x}_{\text{des}}$ 为任务期望速度。引入松弛变量 $\epsilon$ 可处理不可行约束：
\begin{equation}
\min_{\dot{q}, \epsilon} \| J \dot{q} - \dot{x}_{\text{des}} + \epsilon \|^2, \quad \epsilon \ge 0.
\end{equation}

对于多任务情况，可按照优先级依次求解：
\begin{align}
\dot{q}_1^* &= \arg\min_{\dot{q}, \epsilon_1} \| J_1 \dot{q} - \dot{x}_1 + \epsilon_1 \|^2, \\
\dot{q}_2^* &= \arg\min_{\dot{q}, \epsilon_2} \| J_2 \dot{q} - \dot{x}_2 + \epsilon_2 \|^2, \quad \text{s.t. } J_1 \dot{q} = \dot{x}_1^*,
\end{align}
依此类推，得到严格优先级的解 $\dot{q}_k^*$。通过这种分层二次规划方法，可以同时处理等式、不等式约束，并在不可行情况下利用松弛变量保证优化过程可解。最终，结合动力学模型可计算出所需关节力矩，实现多任务轨迹跟踪和任务空间任务完成。


尽管全身控制方法通常依赖精确的系统模型和显式约束求解，其所蕴含的控制思想对于更一般的运动生成问题具有重要启示意义。特别地，任务分解、优先级建模以及在可行空间内协调多目标行为的理念，为理解和设计复杂全身运动策略提供了一种清晰的结构化视角。该思想为后续引入数据驱动和学习方法奠定了重要的理论指导，使得学习到的控制策略能够在隐式层面体现类似的协调与优先级行为。



\section{强化学习}

强化学习（Reinforcement Learning, RL）研究如何通过与环境交互来训练智能体，使其在给定任务中获得最大累积回报。通常，这一过程可建模为马尔可夫决策过程（Markov Decision Process, MDP），其核心包括状态空间 \(S\)、动作空间 \(A\)、环境动力学 \(p(s_{t+1} \mid s_t, a_t)\) 以及奖励函数 \(r(s_t, a_t, s_{t+1})\)。智能体的策略 \(\pi(a_t \mid s_t)\) 定义了在每个状态下选择动作的概率分布。

智能体与环境的交互被组织为若干回合（episode）。在每个回合开始时，智能体从初始状态 \(s_0 \sim p(s_0)\) 出发。随后，在每个时间步 \(t\)，智能体观测当前状态 \(s_t\)，并根据策略采样动作 \(a_t \sim \pi(a_t \mid s_t)\)，然后将该动作作用于环境。环境根据动力学模型转移到下一个状态 \(s_{t+1} \sim p(s_{t+1} \mid s_t, a_t)\)，同时返回标量奖励 \(r_t = r(s_t, a_t, s_{t+1})\)，该奖励量化了状态转移对于任务目标的价值。智能体在一个回合内的交互轨迹可表示为 \(\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \ldots, s_T)\)。

智能体的目标是找到最优策略，使期望折扣回报最大化，即
\begin{equation}
\pi^* = \arg \max_{\pi} J(\pi)
\label{eq:optimal_policy}
\end{equation}
其中 \(J(\pi)\) 表示策略的期望折扣回报：
\begin{equation}
J(\pi) = \mathbb{E}_{\tau \sim p(\tau \mid \pi)} \left[ \sum_{t=0}^{T-1} \gamma^t r_t \right]
\label{eq:expected_return}
\end{equation}
其中 \(\gamma \in [0,1]\) 为折扣因子，用于调节即时奖励与未来奖励的相对权重。通过优化该目标函数，智能体能够学习在环境中实现期望行为的控制策略。

\subsection{值函数}

在强化学习中，值函数是评估策略性能的重要工具，它用于估计在特定状态或动作下，遵循策略 \(\pi\) 所能获得的期望未来回报\cite{SuttonBarto2018}。

\subsubsection{状态值函数与状态-动作值函数}

策略 \(\pi\) 的状态值函数 \(V^{\pi}(s)\) 表示智能体从状态 \(s\) 出发，遵循策略 \(\pi\) 时的期望累积回报：
\begin{equation}
V^{\pi}(s) = \mathbb{E}_{\tau \sim p(\tau \mid \pi, s_0=s)} \Bigg[ \sum_{t=0}^{T-1} \gamma^t r_t \Bigg],
\label{eq:state_value_function}
\end{equation}
其中，\(\gamma \in [0,1]\) 为折扣因子，\(p(\tau \mid \pi, s_0=s) = \prod_{t=0}^{T-1} p(s_{t+1} \mid s_t, a_t) \pi(a_t \mid s_t)\) 表示从状态 \(s\) 出发，遵循策略 \(\pi\) 所生成轨迹 \(\tau\) 的概率。状态值函数可以理解为智能体处于某一状态的期望价值。

状态-动作值函数 \(Q^{\pi}(s, a)\)，通常称为 Q 函数，用于评估在状态 \(s\) 采取动作 \(a\) 并随后遵循策略 \(\pi\) 的期望累积回报：
\begin{equation}
Q^{\pi}(s, a) = \mathbb{E}_{\tau \sim p(\tau \mid \pi, s_0=s, a_0=a)} \Bigg[ \sum_{t=0}^{T-1} \gamma^t r_t \Bigg],
\label{eq:state_action_value_function}
\end{equation}
其中 \(p(\tau \mid \pi, s_0=s, a_0=a) = p(s_1 \mid s_0, a_0) \prod_{t=1}^{T-1} p(s_{t+1} \mid s_t, a_t) \pi(a_t \mid s_t)\)。

在不引起歧义的情况下，值函数和 Q 函数对策略 \(\pi\) 的依赖通常省略。此时，\(V(s)\) 简称为状态值函数，\(Q(s,a)\) 简称为 Q 函数。

\subsubsection{最优值函数与最优策略}

最优策略 \(\pi^*\) 的状态值函数与 Q 函数分别记作 \(V^*(s)\) 和 \(Q^*(s,a)\)。最优策略可以通过选择使 Q 函数最大的动作来恢复：
\begin{equation}
\pi^*(a \mid s) =
\begin{cases}
1, & \text{如果 } a = \arg \max_{a'} Q^*(s, a') \\
0, & \text{否则}
\end{cases}
\label{eq:optimal_policy_from_q}
\end{equation}

\subsubsection{贝尔曼方程}

状态值函数与 Q 函数之间存在递归关系，可通过贝尔曼方程表示：

\begin{equation}
Q^{\pi}(s, a) = \mathbb{E}_{s' \sim p(s' \mid s, a), a' \sim \pi(a' \mid s')} \Big[ r(s, a, s') + \gamma Q^{\pi}(s', a') \Big],
\label{eq:q_bellman_recursive}
\end{equation}

\begin{equation}
Q^{\pi}(s, a) = \mathbb{E}_{s' \sim p(s' \mid s, a)} \Big[ r(s, a, s') + \gamma V^{\pi}(s') \Big],
\label{eq:q_bellman_recursive_v}
\end{equation}

\begin{equation}
V^{\pi}(s) = \mathbb{E}_{a \sim \pi(a \mid s), s' \sim p(s' \mid s, a)} \Big[ r(s, a, s') + \gamma V^{\pi}(s') \Big],
\label{eq:v_bellman_recursive}
\end{equation}

\begin{equation}
V^{\pi}(s) = \mathbb{E}_{a \sim \pi(a \mid s), s' \sim p(s' \mid s, a), a' \sim \pi(a' \mid s')} \Big[ r(s, a, s') + \gamma Q^{\pi}(s', a') \Big].
\label{eq:v_bellman_recursive_q}
\end{equation}

这些递归方程在计算和学习值函数及 Q 函数时非常有用，后续可用于策略评估、策略改进以及强化学习训练中。

\subsection{策略评估（Policy Evaluation）}

在强化学习算法中，学习给定策略 \(\pi\) 的状态值函数或状态-动作值函数通常作为一个基本子过程出现。策略评估（Policy Evaluation）是一种基于动态规划思想的方法，通过利用值函数的递归结构，对策略对应的期望回报进行近似计算\cite{SuttonBarto2018}。

\subsubsection{状态值函数的策略评估}

假设通过在环境中执行策略 \(\pi\) 收集得到一组状态转移样本数据集
\[
\mathcal{D} = \{(s_i, a_i, r_i, s'_i)\}.
\]
策略评估采用迭代方式对状态值函数 \(V^{\pi}\) 进行近似估计，从一个初始估计 \(V_0\) 开始。在第 \(k\) 次迭代中，通过优化以下目标函数得到新的值函数近似 \(V_k\)：
\begin{equation}
V_k = \arg\min_{V} \;
\mathbb{E}_{(s_i, r_i, s'_i) \sim \mathcal{D}}
\Big[ \big( y_i - V(s_i) \big)^2 \Big],
\label{eq:policy_eval_v}
\end{equation}
其中目标值 \(y_i\) 定义为
\begin{equation}
y_i = r_i + \gamma V_{k-1}(s'_i),
\label{eq:policy_eval_v_target}
\end{equation}
该目标值利用上一轮迭代得到的值函数 \(V_{k-1}\) 对状态 \(s_i\) 的期望回报进行估计。

上述方法采用了一步自举（single-step bootstrapping）的方式构造目标值。除了一步自举外，也可以使用多步自举方法，例如 TD(\(\lambda\))\cite{SuttonBarto2018}。在表格型表示的情形下，可以证明当迭代次数 \(k \to \infty\) 时，\(V_k\) 将收敛到真实的状态值函数 \(V^{\pi}\)。在实际应用中，通常只需相对较少的迭代次数即可获得较为准确的近似结果。

\subsubsection{状态-动作值函数的策略评估}

状态-动作值函数 \(Q^{\pi}(s,a)\) 可以通过类似的迭代过程进行估计。在第 \(k\) 次迭代中，通过优化以下目标函数构造 Q 函数的近似：
\begin{equation}
Q_k = \arg\min_{Q} \;
\mathbb{E}_{(s_i, a_i, r_i, s'_i, a'_i) \sim \mathcal{D}}
\Big[ \big( y_i - Q(s_i, a_i) \big)^2 \Big],
\label{eq:policy_eval_q}
\end{equation}
其中目标值 \(y_i\) 定义为
\begin{equation}
y_i = r_i + \gamma Q_{k-1}(s'_i, a'_i),
\label{eq:policy_eval_q_target}
\end{equation}
该目标值通过上一轮迭代得到的 Q 函数 \(Q_{k-1}\) 进行自举计算。与状态值函数的情形类似，这里每条转移样本被扩展为包含后继状态 \(s'_i\) 及其对应动作 \(a'_i\)，其中 \(a'_i\) 由策略 \(\pi\) 在状态 \(s'_i\) 下采样得到。

\subsubsection{函数逼近与优化}

在实际应用中，值函数和 Q 函数通常采用参数化函数逼近器进行表示，例如神经网络。在这种情况下，上述目标函数（式 \eqref{eq:policy_eval_v} 与式 \eqref{eq:policy_eval_q}）可以通过梯度下降等迭代优化方法进行求解。此外，在每一轮策略评估中，通常不需要将优化问题完全求解至收敛，而是仅进行有限次参数更新即可获得有效的近似，这在实践中具有较高的计算效率。

\subsection{策略梯度方法（Policy Gradient Methods）}

在回顾了强化学习的基本概念之后，下面讨论用于求解强化学习问题的具体算法选择。本文主要关注机器人运动控制任务，这类任务通常具有连续动作空间的特点。策略梯度方法（Policy Gradient Methods）是一类特别适用于连续动作空间的问题的强化学习算法\cite{Williams1992, SuttonBarto2018}，同时也易于与神经网络等函数逼近器相结合。

策略梯度方法通过梯度上升的方式，直接对策略的参数进行优化，其目标是最大化策略的期望回报。具体而言，策略参数通过对期望回报关于策略参数的梯度 \(\nabla_{\pi} J(\pi)\) 进行迭代更新来优化。

\subsubsection{策略梯度的推导}

为了推导策略梯度，首先将策略的期望回报（式 \eqref{eq:expected_return}）从基于轨迹分布的形式，改写为基于策略诱导状态分布的期望形式：
\begin{equation}
J(\pi) = \mathbb{E}_{s \sim d^{\pi}(s)} \;
\mathbb{E}_{a \sim \pi(a \mid s)}
\big[ Q^{\pi}(s, a) \big],
\label{eq:pg_objective}
\end{equation}
其中 \(d^{\pi}(s)\) 表示策略 \(\pi\) 所诱导的（未归一化）折扣状态分布，其定义为
\[
d^{\pi}(s) = \sum_{t=0}^{\infty} \gamma^{t} p(s_t = s \mid \pi),
\]
\(p(s_t = s \mid \pi)\) 表示在第 \(t\) 个时间步执行策略 \(\pi\) 时处于状态 \(s\) 的概率。

对式 \eqref{eq:pg_objective} 关于策略参数求梯度，可得
\begin{equation}
\nabla_{\pi} J(\pi)
= \nabla_{\pi}
\mathbb{E}_{s \sim d^{\pi}(s)}
\mathbb{E}_{a \sim \pi(a \mid s)}
\big[ Q^{\pi}(s, a) \big].
\label{eq:pg_grad_1}
\end{equation}
将内层期望写成对动作空间的积分形式，有
\begin{equation}
\nabla_{\pi} J(\pi)
=
\mathbb{E}_{s \sim d^{\pi}(s)}
\left[
\int_{a \in \mathcal{A}}
\nabla_{\pi} \pi(a \mid s) Q^{\pi}(s, a) \, da
\right].
\label{eq:pg_grad_2}
\end{equation}

然而，在高维或连续动作空间中，上述积分形式往往难以直接计算。为此，引入对数导数技巧（score function），将积分形式转化为基于采样的期望：
\begin{equation}
\nabla_{\pi} J(\pi)
=
\mathbb{E}_{s \sim d^{\pi}(s)}
\mathbb{E}_{a \sim \pi(a \mid s)}
\big[
\nabla_{\pi} \log \pi(a \mid s)
\, Q^{\pi}(s, a)
\big].
\label{eq:policy_gradient}
\end{equation}

\subsubsection{策略梯度的估计}

式 \eqref{eq:policy_gradient} 给出了策略梯度的一种可计算形式。在实际应用中，可通过在环境中执行当前策略收集样本轨迹来近似该期望。具体而言，在每个时间步计算动作对数概率的梯度 \(\nabla_{\pi} \log \pi(a_t \mid s_t)\)，并使用对应的 \(Q\) 值对其进行加权，随后对所有时间步的加权梯度取平均，即可得到策略梯度的经验估计。

尽管策略梯度方法在形式上较为直接，但在实践中，其梯度估计通常具有较高的方差，从而可能导致学习过程不稳定或收敛缓慢。因此，实际算法中通常需要引入额外的设计以改善训练稳定性。

\subsubsection{基线与优势函数}

一种常见的方差降低策略是在策略梯度估计中引入基线（baseline）\cite{Williams1992, SuttonBarto2018}。通常选取状态值函数 \(V^{\pi}(s)\) 作为基线，并定义优势函数
\begin{equation}
A^{\pi}(s, a) = Q^{\pi}(s, a) - V^{\pi}(s).
\label{eq:advantage_function}
\end{equation}
在引入优势函数后，策略梯度可写为
\begin{equation}
\nabla_{\pi} J(\pi)
=
\mathbb{E}_{s \sim d^{\pi}(s)}
\mathbb{E}_{a \sim \pi(a \mid s)}
\big[
\nabla_{\pi} \log \pi(a \mid s)
\, A^{\pi}(s, a)
\big].
\label{eq:policy_gradient_adv}
\end{equation}

优势函数刻画了在给定状态下，某一动作相对于平均水平的优劣程度。上述更新规则可以理解为：提高在当前状态下带来高于平均回报的动作的概率，同时降低带来低于平均回报的动作的概率。可以证明，引入基线并不会改变策略梯度的期望值，但能够显著降低其方差，从而提升学习效率。

\subsubsection{值函数估计（Value Estimation）}

策略梯度的计算依赖于对策略对应的状态值函数 \(V^{\pi}\) 以及状态-动作值函数 \(Q^{\pi}\) 的估计。在实际算法设计中，针对这两类量的近似方式存在多种选择，不同的设计决策在计算复杂度、偏差与方差之间形成权衡。

一种直接的做法是利用由策略 \(\pi\) 采样得到的数据，同时对 \(V^{\pi}\) 和 \(Q^{\pi}\) 进行函数逼近，并据此估计策略梯度。然而，在连续控制任务中，直接学习高维状态-动作空间中的 Q 函数往往较为困难。基于这一考虑，本文在整个工作中采用一种更为常见且实践效果良好的策略：使用参数化函数逼近器对状态值函数 \(V^{\pi}\) 进行近似，而状态-动作值函数 \(Q^{\pi}(s,a)\) 则通过经验回报进行直接估计。

具体而言，对于在状态 \(s\) 采取动作 \(a\) 后得到的经验回报，使用如下形式的经验估计：
\[
Q^{\pi}(s,a) \approx R^{\pi}_{s,a},
\]
其中 \(R^{\pi}_{s,a}\) 表示在策略 \(\pi\) 下，从状态 \(s\) 执行动作 \(a\) 开始的折扣累计回报。该经验回报可以通过沿着以 \((s,a)\) 为起点的轨迹，对后续奖励进行折扣求和得到，即
\[
R^{\pi}_{s,a} = \sum_{t=0}^{T-1} \gamma^{t} r_t,
\]
其定义与式 \eqref{eq:q_function_def}（即状态-动作值函数的期望形式）一致。

在这种设置下，状态值函数 \(V^{\pi}\) 主要用于作为策略梯度中的基线，以降低梯度估计的方差；而 Q 函数则通过经验回报的方式隐式参与策略更新。这种估计方式在保持算法简单性的同时，避免了显式学习高维 Q 函数所带来的不稳定性，因此被广泛应用于基于策略梯度的连续控制方法中。

\subsubsection{训练方法（Training）}

在后续章节中，本文主要采用近端策略优化（Proximal Policy Optimization, PPO）算法\cite{Schulman2017PPO}对控制策略进行训练。PPO 属于策略梯度方法的一种，通过在目标函数中引入约束项，有效提升了训练过程的稳定性与样本利用效率，因此在连续控制任务中得到了广泛应用。

算法 \ref{alg:ppo_training} 总结了本文中所采用的整体训练流程。策略函数与状态值函数均采用神经网络进行参数化建模。在每一次参数更新迭代中，首先使用当前策略 \(\pi\) 在环境中采样得到一批轨迹数据 \(\{\tau_i\}_{i=1}^{m}\)。随后，利用采集到的数据对状态值函数 \(V(s)\) 进行更新，其目标值通过 TD(\(\lambda\)) 方法\cite{SuttonBarto2018}计算得到。

在完成值函数更新后，对轨迹中每一个时间步估计其优势函数
\[
A_i \approx A^{\pi}(s_i, a_i),
\]
其中优势函数采用广义优势估计（Generalized Advantage Estimation, GAE(\(\lambda\))）方法\cite{Schulman2016GAE}进行计算。最终，基于估计得到的优势值，通过 PPO 算法对策略网络的参数进行更新。

上述训练流程在保证梯度估计稳定性的同时，兼顾了样本效率与实现复杂度，因此被作为本文后续实验中默认的策略训练方法。

\subsubsection{目标条件强化学习（Goal-Conditioned Reinforcement Learning）}

前述强化学习框架主要针对单一任务场景，即智能体在训练与测试过程中始终试图完成相同的目标。然而，在许多实际应用中，更期望智能体具备执行多种任务的能力。为此，可以采用目标条件强化学习（Goal-Conditioned Reinforcement Learning）框架，对标准强化学习问题进行扩展。

在该框架中，引入目标变量 \(g \in \mathcal{G}\) 用于描述智能体当前需要完成的具体任务。例如，在导航任务中，目标 \(g\) 可以表示需要到达的目标位置坐标。在每个回合开始时，根据任务分布采样一个目标
\[
g \sim p(g),
\]
并使奖励函数依赖于该目标，即
\[
r_t = r(s_t, a_t, s_{t+1}, g)。
\]
同时，目标 \(g\) 作为额外输入与状态信息一并提供给策略函数，使策略表示为
\[
\pi(a \mid s, g)。
\]

在目标条件设置下，策略的期望回报定义为其在目标分布上的平均性能：
\begin{equation}
J(\pi) = \mathbb{E}_{g \sim p(g)} 
\mathbb{E}_{\tau \sim p(\tau \mid \pi, g)}
\left[
\sum_{t=0}^{T-1} \gamma^{t} r_t
\right].
\label{eq:goal_conditioned_objective}
\end{equation}

此时，策略对应的轨迹分布同样依赖于目标 \(g\)，可表示为
\[
p(\tau \mid \pi, g) = p(s_0) \prod_{t=0}^{T-1} p(s_{t+1} \mid s_t, a_t)\,\pi(a_t \mid s_t, g)。
\]
相应地，状态值函数 \(V^{\pi}(s, g)\) 与状态-动作值函数 \(Q^{\pi}(s, a, g)\) 也均以目标 \(g\) 作为附加输入。

需要指出的是，在算法实现层面，目标变量 \(g\) 可以被视为状态观测的一部分，与状态 \(s\) 以完全相同的方式进行处理。因此，前文所讨论的策略梯度方法、值函数估计方法以及 PPO 训练流程，均可以在不作本质修改的情况下，直接推广至目标条件强化学习设置。此外，该框架还可进一步扩展以支持随时间变化的非平稳目标 \(g_t\)。




\section{电机控制}
我们的工作将探索强化学习在运动控制任务中的应用，包括模拟环境和真实环境。然而，直接将强化学习应用于真实环境中的智能体训练会带来诸多挑战，例如样本效率、安全性、复位问题等。为了规避部分挑战，我们先在模拟环境中训练机器人。本节将详细介绍机器人的设计，并对运动控制相关概念进行简要回顾。

\subsection{机器人模型}

\subsection{状态特征}
为了控制角色的运动，策略的输入之一是状态特征集 $s_t$，用于描述角色在时刻 $t$ 的身体配置。图 2.3 给出了状态特征的一个示例，其定义如下：
$$
s_t = \left[ \boldsymbol{\omega}_t,\ r_t,\ p_t,\ \dot{p}_t,\ a_{t-1},\ \beta \right].
$$

其中，$\boldsymbol{\omega}_t \in \mathbb{R}^3$ 表示机器人基座的角速度；$r_t$ 表示基座的横滚角（roll），$p_t$ 表示俯仰角（pitch）；$p_t \in \mathbb{R}^{19}$ 和 $\dot{p}_t \in \mathbb{R}^{19}$ 分别表示机器人各关节的位置与速度；$a_{t-1} \in \mathbb{R}^{19}$ 为上一时间步的动作输入；$\beta \in (0,1]$ 为用于缩放策略输出动作幅值的标量参数。

\subsection{驱动模型}

在每一个时间步，策略通过输出动作 $a_t$ 来控制角色的运动，该动作随后被驱动模型转换为施加在各关节上的控制力矩。对于关节 $j$，其控制力矩记为 $f_j$。动作参数化方式以及驱动模型的选择是强化学习控制中的关键设计决策，对策略的训练稳定性以及最终生成动作的质量均具有重要影响。

一种直接的动作参数化方式是令动作
\begin{equation}
a = (f_1, f_2, \ldots, f_n)
\end{equation}
直接表示各关节的控制力矩，此时驱动模型可视为恒等映射。该方法在文献中被广泛采用，但对于具有多个自由度的复杂关节系统而言，直接进行低层力矩控制往往会增加学习难度，并且容易导致生成动作中出现高频抖动等视觉伪影。

在已有研究中发现，对于全身动作控制任务，引入更高层次的控制抽象（如基于位置的控制）通常能够显著提升学习效率与动作质量。因此，在本文后续章节中，采用比例–微分（Proportional-Derivative, PD）控制器作为驱动模型。

\paragraph{PD 控制器}  
在使用 PD 控制器的情况下，策略输出的动作
\begin{equation}
a = (\hat{q}_1, \hat{q}_2, \ldots, \hat{q}_n)
\end{equation}
表示机器人各关节的目标旋转位置 $\hat{q}_j$。为便于说明，首先考虑一维转动关节的情形。给定目标旋转角 $\hat{q} \in \mathbb{R}$，PD 控制器可建模为一个角弹簧–阻尼系统，其输出的关节力矩 $f$ 表达为：
\begin{equation}
f = k_p (\hat{q} - q) - k_d \dot{q},
\end{equation}
其中，$q$ 和 $\dot{q}$ 分别表示关节当前的旋转角度与角速度，$k_p$ 和 $k_d$ 为比例增益和微分增益参数，其取值可参考图 2.2。

该驱动模型通过施加恢复力矩将关节驱动至目标旋转位置，同时引入阻尼项以抑制关节速度过大，从而在保证控制稳定性的同时生成平滑、自然的运动。










