\chapter{基于模仿学习的全身控制框架}
大规模动作捕捉数据集涵盖了从风格化行走到复杂全身行为在内的多样化、高动态运动模式。若能够将这些人类运动有效迁移至类人机器人，它们将形成一套可复用的运动基元库，从而为多种下游任务提供统一且具有良好泛化能力的控制基础。

类人机器人由于其与人体高度相似的结构和丰富的自由度，天然适合执行这类全身运动并参与人机物理交互。然而，这种结构相似性也使其控制问题远比传统机器人更加复杂：高维状态空间、强关节耦合以及不稳定的接触动力学共同构成了严峻挑战，从而显著制约了这些人类动作在真实机器人上的直接落地。

提升学习型控制器性能的一条自然途径是引入动作捕捉数据或人工设计的动画数据作为运动先验。现有研究通常采用一种分层控制结构，即在运动学动画系统之上叠加一个基于物理的跟踪控制器 \cite{lee2010data}。然而，该类方法存在显著局限性：一方面，运动学层必须生成物理上可被精确跟踪的参考轨迹，否则会导致控制失败或运动质量下降；另一方面，所得的物理控制器在偏离原始参考以实现自恢复或任务导向行为时缺乏足够的灵活性。此外，这类系统在工程实现上通常较为复杂，难以扩展到大规模、多样化动作集。

近年来，基于物理的角色动画研究 \cite{luo2023universal,tessler2024maskedmimic,huang2025diffuse,pan2025tokenhsi} 在将人类动作合成为面向下游任务的动态全身控制策略方面取得了显著进展。然而，这些成果主要建立在仿真环境中，其中智能体通常假设拥有理想化动力学、无限执行能力以及完美的状态观测，这与真实世界中的类人机器人存在本质差异。真实机器人必须面对未建模动力学、执行器与传感器限制以及不完备状态估计等问题，使得直接迁移这些方法往往难以获得稳定可靠的表现。因此，要在真实硬件平台上实现类似水平的运动能力，仍然缺乏两个关键要素：（1）一种可扩展且高质量的运动跟踪框架，能够将运动学参考动作转化为鲁棒且高度动态的真实机器人运动，同时避免传统方法中常见的过度随机化、抖动与运动质量退化问题；（2）一种有效的训练动作采样机制，能够在复杂动作与简单动作之间合理分配训练资源，避免由于难度分布不均导致的泛化性能下降。

针对上述挑战，本文提出了一种面向真实世界的类人机器人全身动作学习框架。该框架首先构建了一条高精度的运动跟踪流水线，使机器人能够在真实硬件上以接近动画级质量执行包括摔倒爬起、坐下起身等在内的高度动态全身动作。与仅进行表观模仿的方法不同，该框架在学习过程中显式建模物理约束与动力学一致性，从而有效抑制了以往深度强化学习方法（如 \cite{duan2016benchmarking}）中常见的滑移、抖动与姿态漂移等运动伪影。在存在外界扰动或对原始动作进行修改时，所生成的运动仍保持自然流畅，且恢复策略表现出高度鲁棒性，无需依赖复杂的人工工程化调节。

\section{相关工作}

类人机器人的物理交互本身具有高度挑战性，其中一个根本原因在于系统动力学的强非线性和高维耦合特性。传统控制方法通常依赖精确的动力学建模与解析控制策略 \cite{chignoli2021humanoid,dallard2023synchronized,sentis2018whole,ramos2019dynamic}，然而在涉及复杂接触与全身协调的场景中，这类方法往往难以兼顾稳定性与灵活性。

近年来，随着强化学习（Reinforcement Learning, RL）和仿真到真实（sim-to-real）迁移技术的快速发展，四足机器人和类人机器人在学习复杂全身技能方面取得了显著进展，尤其在高复杂度和强接触任务中展现出卓越的适应能力与运动性能 \cite{agarwal2023legged,cheng2024extreme,duan2024learning,fu2023deep,ito2022efficient,jeon2023learning,ji2023dribblebot}。这些成果表明，基于数据驱动的方法能够有效缓解精确建模带来的局限。

在类人机器人领域，一些研究进一步将 RL 应用于全身控制与动作生成 \cite{cheng2024expressive,fu2024humanplus}，尝试提升机器人的动作表现力并支持操作与模仿学习等任务。尽管这些方法已在一定程度上实现了更自然的运动风格，但在动作多样性与机动性方面仍存在明显限制，表明其尚难全面覆盖真实世界中丰富多变的运动需求。

与此相对应，RL 在计算机图形学与虚拟角色控制领域已经展现出极强的动作生成能力，例如在人体动作模仿 \cite{peng2018deepmimic}、多任务技能学习 \cite{peng2022ase} 以及基于视觉输入的人体动作跟踪 \cite{18} 等方面均取得了显著成功。同时，在真实硬件平台上，RL 也已被用于实现稳定而灵活的双足行走 \cite{radosavovic2023learning,siekmann2021blind}，进一步验证了其在高维动力学控制问题中的潜力。

然而，纯强化学习方法通常依赖复杂的奖励设计和精细的超参数调节，在面对多样化动作和任务需求时，训练代价高昂且可扩展性受限。为缓解这一问题，将 RL 与人体运动数据作为先验知识相结合已成为一种有效范式。例如，H2O \cite{he2024learning} 通过将 RL 与遥操作示例相结合，实现了全尺寸类人机器人的全身控制，而 OmniH2O \cite{he2024omnih2o} 进一步引入多种控制接口，以支持更加复杂和多样化的交互任务。

在动作表现力方面，ExPressive \cite{cheng2024expressive} 通过 RL 学习下肢行走策略，并结合上肢运动跟踪，实现了上半身模仿与下半身运动控制的协同。然而，这类依赖示例数据的动作跟踪方法受限于动作捕捉数据的获取成本与规模，在真实类人机器人硬件上实现同时具备**高运动质量**与**高度动态技能支持**的多动作跟踪尚未被成功展示。本节旨在填补这一空白，通过跟踪包含多种风格与难度、持续数分钟的人类参考动作序列，实现多样化、高动态的人形机器人运动复现。


\section{方法}
我们将类人机器人的全身运动控制建模为一个\emph{目标条件化（goal-conditioned）}的策略学习问题，其策略形式化表示为
$\pi : G \times S \rightarrow A ,$
其中 $G$ 表示用于描述期望行为的目标空间，$S$ 为观测空间，$A$ 为动作空间，对应机器人关节的控制指令（如关节位置或力矩）。在本文其余部分中，在不失一般性的前提下，我们基于 Droid 类人机器人平台对观测空间与动作空间进行具体定义。需要指出的是，所提出的方法同样适用于具有相似身体结构、但在自由度数量或驱动形式上有所差异的类人机器人系统。




\subsection{运动跟踪目标}
我们从经重定向处理后的参考动作出发，该动作由一系列广义位置和速度关键帧 $(\mathbf{q}_m, \mathbf{v}_m)$ 表示。通过正向运动学，对于每一个刚体 $b \in \mathcal{B}$，可以计算其对应的位姿 $T_{b,m}$ 以及空间速度（twist）$V_{b,m}$，其中 $\mathcal{B}$ 表示机器人所有刚体的集合。

在真实硬件执行过程中，由于外部扰动以及仿真到现实（sim-to-real）差异的存在，参考动作在全局坐标系下往往会产生不可避免的漂移。为了在允许一定全局偏移的同时保持动作的相对结构与风格，控制器不应直接跟踪所有刚体的绝对位姿。

为此，我们选取一个锚定刚体 $b_{\text{anchor}} \in \mathcal{B}$（通常为根部或躯干），并将参考动作重新锚定到该刚体的期望状态上。具体而言，对于锚定刚体，直接采用参考动作中的位姿，即 $\hat{T}_{b_{\text{anchor}}} = T_{b_{\text{anchor}},m}$。

对于其余刚体 $b \in \mathcal{B} \setminus \{b_{\text{anchor}}\}$，其期望位姿定义为 $\hat{T}_b = T_\Delta \, T^{-1}_{b_{\text{anchor}},m} \, T_{b,m}$，其中对齐变换 $T_\Delta = (p_\Delta, R_\Delta)$，并满足
$p_\Delta = [p_{b_{\text{anchor}}}.x,\ p_{b_{\text{anchor}}}.y,\ p_{b_{\text{anchor}}}.z_m]$，
以及
$R_\Delta = R_z\!\left(\mathrm{yaw}\!\left(R_{b_{\text{anchor}}} R^{\top}_{b_{\text{anchor}},m}\right)\right)$。

该对齐变换在保持身体高度不变的同时，对齐参考动作与当前机器人的偏航角，并将参考动作的平面位置映射至机器人下方，从而将全局参考动作转化为局部可执行的跟踪目标。

对于空间速度，我们直接采用参考动作中的速度信息，即对所有 $b \in \mathcal{B}$，有 $\hat{V}_b = V_{b,m}$。

由于机器人包含大量空间位置相近的刚体，对所有刚体进行跟踪既计算开销较大，也缺乏必要性。因此，我们选取一个用于运动约束的目标刚体子集 $\mathcal{B}_{\text{target}} \subseteq \mathcal{B}$，并将最终的运动跟踪目标定义为
$g_{\text{tracking}} = \{\hat{T}_{b_{\text{anchor}}},\ \hat{T}_b,\ \hat{V}_b\},\ \forall b \in \mathcal{B}_{\text{target}}$。

\subsection{观测}
我们将策略的观测空间拆解为一个单时间步向量，由以下两个部分组成。
\textbf{(1) 参考相位信息。}
参考相位。我们引入参考动作中的关节位置和关节速度$c = [q_{\text{joint},m}, \, v_{\text{joint},m}],$
该信息仅用于提供相位参考，策略并不被要求直接跟踪这些关节数值。

\textbf{(2) 本体感知信息。}
其他本体感知信息。我们进一步包含机器人在根坐标系下根节点的重力投影 $g_t \in \mathbb{R}^3$、根节点角速度 $\omega_t \in \mathbb{R}^3$，以及关节位置 $q_{\text{joint}}$、关节速度 $v_{\text{joint}}$，和上一时刻的动作 $a_{\text{last}}$。

综合上述各项，完整的观测向量表示为
\begin{equation}
    o = [c,\;g_t,\;\omega_t,\; q_{\text{joint}},\; v_{\text{joint}},\; a_{\text{last}} ].
\end{equation}
以上观测能够避免了一般学习任务中需要依靠状态估计，估计出线速度相关分量以及计算出锚点的位置，重而降低观测复杂性。

\subsection{奖励设计}
我们以一种简单、直观且具有通用性的方式设计奖励函数，其由两部分组成：(1) 任务奖励，作为正向奖励，采用统一权重并在任务空间中定义；(2) 最小化的正则化惩罚项，用于避免对跟踪性能造成负面影响。任务奖励由身体跟踪奖励构成。首先，针对每一个目标刚体 $b \in B_{\text{target}}$，基于期望的位姿与速度 $(\hat{T}_b, \hat{V}_b)$ 以及实际的位姿与速度 $(T_b, V_b)$ 计算误差度量：位置误差 $e_{p,b} = \hat{p}_b - p_b$，姿态误差 $e_{R,b} = \log(\hat{R}_b R_b^\top)$，线速度误差 $e_{v,b} = \hat{v}_b - v_b$，以及角速度误差 $e_{w,b} \approx \hat{w}_b - w_b$（在姿态误差较小的假设下）。随后，在所有目标刚体上计算各类误差的均方值：$\bar{e}_\chi = \frac{1}{|B_{\text{target}}|} \sum_{b \in B_{\text{target}}} \| e_{\chi,b} \|^2$，其中 $\chi \in \{p, R, v, w\}$。每一种误差度量随后通过高斯形式的指数函数进行归一化：$r(\bar{e}_\chi, \sigma_\chi) = \exp\!\left(-\bar{e}_\chi / \sigma_\chi^2\right)$，其中 $\sigma_\chi$ 为通过经验确定的名义误差尺度。综合后的任务奖励定义为：$r_{\text{task}} = \sum_{\chi \in \{p, R, v, w\}} r(\bar{e}_\chi, \sigma_\chi)$。在正则化方面，我们仅引入对仿真到真实迁移至关重要的三个惩罚项。其中，关节限位惩罚 $r_{\text{limit}}$ 用于鼓励关节位置保持在软限位范围内；动作变化率惩罚 $r_{\text{smooth}}$ 用于鼓励相邻时间步的动作平滑变化，从而避免策略产生过度抖动；自碰撞惩罚 $r_{\text{contact}}$ 通过统计自接触力超过预设阈值的刚体数量来计算，该统计在 $b \notin B_{\text{ee}} \subseteq B$ 的刚体集合上进行，其中 $B_{\text{ee}}$ 表示末端执行器刚体集合。最终的总奖励形式为：$r = r_{\text{task}} - \lambda_l r_{\text{limit}} - \lambda_s r_{\text{smooth}} - \lambda_c r_{\text{contact}}$，其中 $\lambda_l, \lambda_s, \lambda_c > 0$ 为各正则化项对应的权重系数。作为可选项，还可以为锚定刚体 $b_{\text{anchor}}$ 添加一个全局跟踪奖励，其结构与 $r_{\text{tracking}}$ 相同，但仅使用位置误差 $e_{p,b_{\text{anchor}}}$ 和姿态误差 $e_{R,b_{\text{anchor}}}$ 来计算。

\subsection{难度采样}
训练包含长时间序列的运动时不可避免地会面临一个问题：不同时间段的难度并不均衡。因此，像以往工作 \cite{14,34,17} 中常用的那样，对整条轨迹进行均匀采样，往往会对简单片段过度采样、而对困难片段采样不足，从而导致奖励方差增大并降低训练效率。基于这一观察，一个自然的想法是更加频繁地从困难区域进行采样。为此，我们将整段运动轨迹的起始索引划分为 $S$ 个 bin，每个 bin 对应 1 秒的时间长度，并根据经验性的失败统计信息对这些 bin 进行采样。设 $N_s$ 和 $F_s$ 分别表示从第 $s$ 个 bin 开始的回合数和失败次数。为防止由于短期波动导致采样概率出现离散跳变，我们对失败率采用指数滑动平均进行时间平滑处理。考虑到失败更可能由终止前不久采取的次优动作引起，我们进一步对失败率施加一个非因果卷积，并使用指数衰减核 $k(u)=\gamma^u$（其中 $\gamma$ 为衰减系数），以便对更接近当前时刻的失败赋予更大的权重。最终，从第 $s$ 个 bin 进行采样的概率定义为
\[
p_s=\frac{\sum_{u=0}^{K-1}\alpha^u \bar r_{s+u}}{\sum_{j=1}^{S}\sum_{u=0}^{K-1}\alpha^u \bar r_{j+u}},
\]
其中 $\bar r_s$ 表示第 $s$ 个 bin 的平滑失败率。为了在优先关注困难片段的同时保持对简单片段的覆盖，并缓解灾难性遗忘问题，我们将上述概率 $p_s$ 与均匀分布进行混合，得到 $p'_s=\lambda\frac{1}{S}+(1-\lambda)p_s$，其中 $\lambda$ 为均匀采样比例。最终，起始 bin 从多项分布 $\text{Multinomial}(p'_1,\ldots,p'_S)$ 中采样，从而在整体训练过程中优先覆盖更具挑战性的区域。

\section{讨论}
本节通过仿真以及在 Droid 人形机器人上的真实世界部署，对所提出的运动控制框架进行了实验验证，结果表明该方法能够对全身动作进行高鲁棒和精准跟踪。需要说明的是，在本节中，我们对所有策略均使用同一组超参数进行训练。

% \subsection{Sim-to-Sim 性能评估}
我们使用由 Unitree 重定向的 LAFAN1 数据集~\cite{harvey2020robust}，该数据集包含多种敏捷且多样的人体运动，例如冲刺、转身跳跃和爬行，同时还包含来自先前工作~\cite{zhang2025hub,he2025asap} 的短时单动作片段。LAFAN1 数据集包含 40 条时长为数分钟的参考序列，每一条都属于一个较大的动作类别，并在该类别下包含多种不同的具体动作。我们从中随机选择了 25 条参考序列，并确保每个类别至少包含一条。实验结果表明，在 sim-to-sim 评估中，这些参考序列均能够完整执行。由于测试场地空间受限，我们进一步从这些参考序列中有意挑选了 29 条具有挑战性的动作片段，这些片段包含高度动态且接触频繁的行为，并将其部署到真实硬件上，以评估 sim-to-real 的迁移性能。所有用于 sim-to-sim 以及 sim-to-real 评估的动作列表详见表~I。


\subsection{基线方法}
为系统评估所提出方法的有效性与设计选择的合理性，我们选取以下两类典型设置作为对比基线。

\subsubsection{不同跟踪目标}
在运动跟踪任务中，跟踪目标的表示形式对于实现稳定且可靠的仿真到现实（sim-to-real）迁移至关重要。一种常见选择是采用局部关节空间表示进行跟踪，该表示具有维度低、结构紧凑以及计算效率高等优点，因此在实时控制系统中具有较强的实用性。然而，关节空间表示难以直接刻画末端或身体关键部位在笛卡尔空间中的几何约束。

相比之下，直接对机器人各身体部位的空间位姿进行跟踪能够在笛卡尔空间中提供明确的几何约束，有助于保持整体动作结构与空间一致性。但该方式通常需要更高维度的状态表示，并引入更大的模型规模和更高的推理开销，从而对实时性能提出更高要求。上述两种跟踪目标在计算效率与空间可解释性之间形成了一种权衡关系，由此引出了一个关键问题：哪一种跟踪目标形式更有利于实现高质量的 sim-to-real 迁移。

为回答这一研究问题，我们对两种不同的状态表示方式进行了对比分析，具体如下：

\begin{itemize}
    \item \textbf{Body-Pos State：}
    \begin{itemize}
        \item \textbf{全局状态（Global States）：} 根节点位置（$\mathbb{R}^3$）、线速度（$\mathbb{R}^3$），以及以旋转向量形式表示的朝向（$\mathbb{R}^3$），均相对于当前时刻的角色坐标系表示。
        \item \textbf{局部状态（Local States）：} 所有刚体在角色坐标系下的笛卡尔位置（$\mathbb{R}^{3B}$）及其线速度（$\mathbb{R}^{3B}$），其中 $B$ 表示刚体数量。
    \end{itemize}

    \item \textbf{Joint-Pos State：}
    \begin{itemize}
        \item \textbf{全局状态（Global States）：} 与 Body-Pos State 相同，包括根节点位置、线速度及旋转向量形式的朝向。
        \item \textbf{局部状态（Local States）：} 关节角度（$\mathbb{R}^{J}$）及关节角速度（$\mathbb{R}^{J}$），其中 $J$ 表示关节数量。
    \end{itemize}
\end{itemize}

表~\ref{tab:state_ablation} 总结了两种状态表示在两类任务（行走+扰动与舞蹈+扰动）中的成功率表现。实验结果表明，\textbf{Body-Pos 状态表示在两项任务中均显著优于 Joint-Pos 表示}。

\begin{table}[t]
\centering
\caption{不同状态表示在不同任务下的成功率对比}
\label{tab:state_ablation}
\begin{tabular}{lcc}
\toprule
\textbf{状态表示} & \textbf{行走+扰动} & \textbf{舞蹈+扰动} \\
\midrule
Body-Pos State  & 100\% & 90\% \\
Joint-Pos State & 70\%  & 0\%  \\
\bottomrule
\end{tabular}
\end{table}

尽管从理论上看，基于关节角度的表示更符合 Markov 性假设，但在实际应用中，关节位置估计中的微小误差会沿运动学链逐级累积，使得该表示方式在扩散预测过程中更容易产生误差放大。相比之下，直接预测笛卡尔空间中的身体位姿能够有效缓解该问题，从而表现出更高的稳定性与鲁棒性。


\subsubsection{不同观测设计}
观测空间的设计在强化学习训练过程中同样起着决定性作用，其直接影响策略的学习效率与任务成功率。在确定跟踪目标之后，是否引入历史观测信息、以及是否对参考动作进行前瞻预测，都会显著影响策略网络的表达能力与泛化性能。

为此，我们设计并比较了以下三种典型观测配置：
（1）不使用任何历史信息，仅采用单时刻观测；
（2）对参考动作进行 $5$ 帧的前瞻预测，同时对机器人本体感觉信息采用历史堆叠；
（3）对所有观测量（包括参考动作与本体感觉信息）均采用历史堆叠。

通过上述基线设置的对比实验，我们系统分析不同观测设计对学习稳定性、跟踪精度以及 sim-to-real 迁移性能的影响。

进一步地，我们对不同观测历史设计方式进行了消融分析，具体包括：
\begin{enumerate}
    \item \textbf{无历史信息：} 所有观测仅使用当前时刻信息；
    \item \textbf{参考预测 + 本体历史堆叠：} 对参考动作预测未来 $5$ 帧，本体状态采用历史堆叠；
    \item \textbf{统一历史堆叠：} 所有观测（参考动作与本体状态）均采用历史堆叠。
\end{enumerate}

表~\ref{tab:history_ablation} 给出了不同观测设计下的训练稳定性与收敛特性。

\begin{table}[t]
\centering
\caption{不同观测历史设计的训练稳定性对比}
\label{tab:history_ablation}
\begin{tabular}{lccc}
\toprule
\textbf{观测方式} & \textbf{迭代次数} & \textbf{Loss 爆炸} \\
\midrule
无历史信息 & 12000 & 否 \\
参考预测 + 本体历史堆叠 & 4000 & 是 \\
统一历史堆叠 & 8000 & 否 \\
\bottomrule
\end{tabular}
\end{table}

从表中可以看出，统一采用历史堆叠的方式虽然在收敛速度上不及使用参考预测的方法，但其训练过程更加稳定。相比之下，引入参考动作预测虽然能够加快早期学习速度，但容易导致策略过拟合，从而使训练过程中的损失函数迅速增大，最终导致学习失败。

另一方面，相较于不使用历史信息的设置，历史堆叠能够使策略显式建模时间相关性，从而提升对动力学不确定性和观测噪声的鲁棒性，这对于 sim-to-real 迁移尤为重要。

\subsection{评价指标}
我们将我们的方法与其他全身控制方法进行对比，包括ASAP，PBHC，H2O等，为了更好的说明效果，我们定义一下评价指标
成功率：我们报告策略的 \textbf{成功率（success rate）}，当在任意时刻机器人平均身体部位距离误差超过 $0.5\,\mathrm{m}$ 时，认为该次模仿任务失败。

此外，我们通过以下指标评估策略对参考运动的跟踪能力：
\begin{itemize}
    \item 全局平均刚体位置误差 $E_{\text{g-mpdpe}}$（mm）；
    \item 相对于根节点的平均刚体位置误差 $E_{\text{mpdpe}}$（mm）；
    \item 加速度误差 $E_{\text{acc}}$（mm/frame$^2$）；
    \item 根节点速度误差 $E_{\text{vel}}$（mm/frame）。
\end{itemize}

全局平均刚体位置误差用于衡量各刚体在\textbf{全局坐标系}下的平均位置偏差，其定义为：
\begin{equation}
E_{\text{g-mpbpe}} = \mathbb{E}\Big[ \big\| \mathbf{p}_t - \mathbf{p}^{\text{ref}}_t \big\|_2^2 \Big],
\label{eq:eg-mpbpe}
\end{equation}
其中 $\mathbf{p}_t$ 表示当前时刻各刚体在全局坐标系下的位置，$\mathbf{p}^{\text{ref}}_t$ 表示对应的参考刚体位置。

相对于根节点的平均刚体位置误差用于衡量在\textbf{根节点坐标系}下的刚体位置偏差，其定义为：
\begin{equation}
E_{\text{mpbpe}} = \mathbb{E}\Big[ 
\big\| (\mathbf{p}_t - \mathbf{p}_{\text{root},t}) - (\mathbf{p}^{\text{ref}}_t - \mathbf{p}^{\text{ref}}_{\text{root},t}) \big\|_2^2
\Big],
\label{eq:empbpe}
\end{equation}
其中 $\mathbf{p}_{\text{root},t}$ 和 $\mathbf{p}^{\text{ref}}_{\text{root},t}$ 分别表示当前运动与参考运动在时刻 $t$ 的根节点位置。


上述指标的最终结果均在所有测试运动序列上取平均值。

\begin{table*}[t]
\centering
\caption{不同方法在不同难度等级下的性能对比结果（均值 $\pm$ 标准差，$\downarrow$ 表示越小越好）}
\label{tab:main_results_cn}
\resizebox{\textwidth}{!}{
\begin{tabular}{lccccc}
\toprule
\textbf{难度} &
$\mathbf{Succ\uparrow}$ &
$\mathbf{E_{g_{mpbpe}}\downarrow}$ &
$\mathbf{E_{mpdpe}\downarrow}$ &
$\mathbf{E_{acc}\downarrow}$ &
$\mathbf{E_{vel}\downarrow}$ \\
\midrule
\multicolumn{6}{l}{\textbf{简单}} \\
\midrule
OmniH2O  & 92$\pm$0.00 & 233.54$\pm$4.013 & 103.67$\pm$1.912 & 4.50$\pm$0.0228 & 5.56$\pm$0.031 \\
ASAP & 97$\pm$0.00 & 108$\pm$0.678 & 49.4$\pm$0.49 & 2.74$\pm$0.025 & 4.46$\pm$0.020 \\
PBHC & 99$\pm$0.00 & 53.25$\pm$17.60 & 46.4$\pm$0.269 & 2.63$\pm$0.012 & 4.59$\pm$0.021 \\
Ours & 100$\pm$0.00 & 41.79$\pm$1.71 & 21.86$\pm$2.0 & 2.35$\pm$0.017 & 4.23$\pm$0.031 \\
\midrule
\multicolumn{6}{l}{\textbf{中等}} \\
\midrule
OmniH2O  & 85$\pm$0.00 & 433.64$\pm$16.22 & 151.42$\pm$7.340 & 4.83$\pm$0.02 & 5.69$\pm$0.019 \\
ASAP & 96$\pm$0.00 & 126.48$\pm$27.01 & 48.87$\pm$7.550 & 2.53$\pm$0.019 & 4.45$\pm$0.026 \\
PBHC & 98$\pm$0.00 & 120.57$\pm$17.01 & 36.05$\pm$4.150 & 2.67$\pm$0.022 & 4.64$\pm$0.031 \\
Ours & 100$\pm$0.00 & 110.91$\pm$13.40 & 41.4$\pm$6.01 & 1.93$\pm$0.0232 & 3.59$\pm$0.31 \\
\midrule
\multicolumn{6}{l}{\textbf{困难}} \\
\midrule
OmniH2O  & 66$\pm$0.00 & 446.17$\pm$12.84 & 147.88$\pm$4.142 & 4.91$\pm$0.064 & 6.88$\pm$0.064 \\
ASAP & 94$\pm$0.00 & 108$\pm$0.678 & 46.4$\pm$0.269 & 3.72$\pm$0.036 & 6.52$\pm$0.042 \\
PBHC & 96$\pm$0.00 & 290.36$\pm$139.1 & 124.61$\pm$53.54 & 3.86$\pm$0.041 & 6.71$\pm$0.024 \\
Ours & 100$\pm$0.00 & 140$\pm$2.762 & 106.4$\pm$0.161 & 2.93$\pm$0.012 & 4.39$\pm$0.021 \\
\bottomrule
\end{tabular}
}
\end{table*}

如表~\ref{tab:main_results_cn} 所示，我们的方法在三种难度等级（简单、中等和困难）下均取得了最优或近似最优的整体性能表现。具体而言，在所有难度设置中，我们的方法始终保持 $100\%$ 的成功率，并在全局平均刚体位置误差 $E_{\text{g-mpbpe}}$、相对于根节点的平均刚体位置误差 $E_{\text{mpbpe}}$、加速度误差 $E_{\text{acc}}$ 以及根节点速度误差 $E_{\text{vel}}$ 等关键指标上稳定优于 OmniH2O 和 ASAP 等基线方法，尤其在中等和困难难度下优势更加明显。

上述性能提升主要归因于我们提出的整体框架设计与难度自适应采样机制。该机制能够根据不同动作片段的动态特性自动调整跟踪难度与相关控制因子，从而在保证训练稳定性的同时，引导策略更加关注具有挑战性的动作区域。相比之下，现有基线方法通常依赖固定或基于经验调节的超参数设置，这类参数在面对动作复杂度和动力学特性高度多样化的运动序列时，往往难以实现良好的泛化能力，导致在高难度场景中性能显著退化。

在可部署的基线方法中，PBHC 在多数指标上表现出较强的竞争力，并在简单和中等难度下接近 oracle 级别的性能。然而，其在困难难度下的成功率和误差稳定性仍明显落后于我们的方法，表明其在应对复杂全身动作和长期时序依赖时仍存在一定局限。

% 需要指出的是，尽管 MaskedMimic 在部分指标上展现出较优的数值表现，但该方法主要面向角色动画生成任务，其设计未充分考虑机器人控制中至关重要的约束条件，例如部分可观测性、动作连续性以及执行平滑性等。因此，MaskedMimic 并不适用于真实机器人部署场景。在本文中，我们将其视为一种 \emph{oracle-style} 的参考下界，用于刻画理论上可达到的性能水平，而非与可部署方法进行直接公平对比的基线。

% 不同方法在各难度等级下的性能对比结果。PBHC 是所有可部署基线方法中表现最优的方法，并在部分指标上接近 oracle 级别性能。实验结果以均值 $\pm$ 一个标准差的形式报告；在排除 MaskedMimic 的情况下，加粗结果表示性能处于最佳方法一个标准差范围内。





\section{sim2real}
我们将采用所提出方法训练得到的运动跟踪策略部署到 Droid 人形机器人上。所有底层电机控制代码均使用 C++ 编写，并针对实时执行进行了优化。底层电机控制层以1Khz进行更新，上层策略以50hz进行更新。值得注意的是，实验过程中未使用任何外部动作捕捉系统，所有策略均通过 ONNX Runtime~\cite{onnxruntime} 在机器人本体 CPU 上执行，每一步推理时间均小于 1.0~ms，从而能够无缝集成到实时状态估计与控制回路中。
系统以 500~Hz 的频率进行全状态估计，采用低层广义动量观测器与卡尔曼滤波器相结合的方法~\cite{flayols2017experimental}。值得注意的是，实验过程中未使用任何外部动作捕捉系统。对于极端且接触频繁的行为（例如从地面站起），我们要么引入激光雷达-惯性里程计（LIO）~\cite{koide2024glim} 进行位置修正，要么完全移除依赖状态估计的观测项。所有策略均通过 ONNX Runtime~\cite{onnxruntime} 在机器人本体 CPU 上执行，每一步推理时间均小于 1.0~ms，从而能够无缝集成到实时状态估计与控制回路中。

